The plots show the distribution of the number of account types (`account_type_count`) at the IPID level for different groups: `consumer`, `high net worth`, `affluent`, and `prospects`. Here, `IPID` represents a level below the household level, meaning a single household can have multiple IPIDs, each corresponding to different account types. Therefore, the `account_type_count` indicates how many different IPID account types a household has.

Key observations:

1. **Violin Plot**:
    - **Consumers**: The distribution is tightly clustered around lower account type counts, typically between 1 and 5.
    - **High Net Worth**: The distribution is more spread out, with a wider range of account type counts, and some households having up to 20 different account types.
    - **Affluent**: Similar to high net worth, but slightly less spread out, with account type counts up to around 15.
    - **Prospects**: The distribution is also more spread out, with account type counts ranging from 1 to about 15, resembling the distribution of affluent customers.

2. **Box Plot**:
    - **Consumers**: Most households have 1 to 5 account types, with few outliers.
    - **High Net Worth**: Households typically have between 5 and 15 account types, with several outliers having up to 20 account types.
    - **Affluent**: The range is similar to high net worth but with fewer extreme outliers.
    - **Prospects**: Households generally have between 5 and 15 account types, with a few outliers above 15.

### Summary:
- **High Net Worth and Affluent** groups tend to have a higher number of account types at the IPID level compared to Consumers and Prospects.
- **Consumers** have a more concentrated distribution with lower account type counts.
- **Prospects** show a distribution pattern similar to Affluent, indicating a higher diversity of account types compared to Consumers.




The plots show the distribution of `balance_trend` for different groups: `consumer`, `high net worth`, `affluent`, and `prospects`. The `balance_trend` represents the slope of the balance change from this month compared to the previous month, averaged over six months.

Key observations:
1. **High Net Worth**: This group is more likely to have extreme values of `balance_trend`, indicating significant fluctuations in their balances.
2. **Consumer, Affluent, and Prospects**: These groups have a more concentrated distribution of `balance_trend`, with fewer extreme values compared to the high net worth group.

Explanation of `balance_trend`: It is the slope of the balance change from this month compared to the previous month, averaged over six months (`这个月的balance对比上个月balance，它的变化的slope，然后是用6个月的数据去做计算做的平均值`).





This image shows the distribution of `state_count` across four groups: `consumer`, `high net worth`, `affluent`, and `prospects`. The `state_count` indicates the number of different states covered by a household account, i.e., how many sub-accounts under one main account include different states.

Key points from the image are:

1. The most frequent `state_count` for all four groups is 1.
2. The `high net worth` and `affluent` groups occasionally exhibit higher `state_count` values, such as 12, 15, and 16.
3. The `prospects` group's distribution is similar to that of the `consumer` group since prospects are derived from consumers; thus, they do not exhibit very high `state_count` values.
4. If `state_count` is greater than 5, it invariably belongs to private wealth groups, either `high net worth` or `affluent`.




# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, lag, max as spark_max, count, when, date_add, lit
from pyspark.sql.window import Window

# Initializing Spark session
spark = SparkSession.builder.appName("EDA_NonPWM_to_PWM").getOrCreate()

# Load data
df = spark.read.csv("path_to_your_data.csv", header=True, inferSchema=True)

# Convert date columns to date type
df = df.withColumn("business_date", to_date(col("business_date"), "yyyy-MM-dd"))
df = df.withColumn("open_date", to_date(col("open_date"), "yyyy-MM-dd"))
df = df.withColumn("close_date", to_date(col("close_date"), "yyyy-MM-dd"))

# Set the reference date
reference_date = "2023-12-31"
reference_date_col = to_date(lit(reference_date), "yyyy-MM-dd")

# Define the time windows
past_6_months_start = date_add(reference_date_col, -180)
future_6_months_end = date_add(reference_date_col, 180)

# Filter for relevant dates
df = df.filter((col("business_date") <= future_6_months_end) & (col("business_date") >= past_6_months_start))

# Determine if the consumer was non-PWM for the entire past 6 months
windowSpec = Window.partitionBy("hh_id_in_wh").orderBy(col("business_date"))
df = df.withColumn("prev_seg_code", lag("seg_code").over(windowSpec))

# Identify consumers constantly non-PWM in the past 6 months
non_pwm_past = df.filter((col("business_date") <= reference_date_col) & (col("business_date") >= past_6_months_start) & (col("seg_code") != "PWM")) \
    .groupBy("hh_id_in_wh").agg(count("seg_code").alias("non_pwm_count"))

# Ensure the consumer has records for the full 6 months (180 days)
non_pwm_past = non_pwm_past.filter(col("non_pwm_count") == 180)

# Identify consumers who switch to PWM in the future 6 months
switch_to_pwm_future = df.filter((col("business_date") > reference_date_col) & (col("business_date") <= future_6_months_end) & (col("seg_code") == "PWM")) \
    .select("hh_id_in_wh").distinct()

# Valid switches are those who are non-PWM in the past 6 months and switch to PWM in the future 6 months
valid_switches = non_pwm_past.join(switch_to_pwm_future, "hh_id_in_wh", "inner")

# Extract the necessary data for analysis
df_switch = df.join(valid_switches, "hh_id_in_wh", "inner").filter(col("business_date") > reference_date_col)
df_switch_pd = df_switch.toPandas()

# Overview of the data
print("Data Overview:")
print(df_switch_pd.info())
print(df_switch_pd.describe())

# Plotting distribution of account longevity
plt.figure(figsize=(10, 6))
sns.histplot(df_switch_pd['account_longevity'], bins=30, kde=True)
plt.title("Distribution of Account Longevity")
plt.xlabel("Account Longevity (days)")
plt.ylabel("Frequency")
plt.show()

# Balance features analysis
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_switch_pd[['curr_bal_amt', 'ledger_bal_amt']])
plt.title("Boxplot of Current Balance and Ledger Balance Amounts")
plt.ylabel("Balance Amount")
plt.show()

# Product diversity analysis
plt.figure(figsize=(10, 6))
sns.histplot(df_switch_pd['product_diversity'], bins=30, kde=True)
plt.title("Distribution of Product Diversity")
plt.xlabel("Number of Distinct Products Used")
plt.ylabel("Frequency")
plt.show()

# Transaction count analysis
plt.figure(figsize=(10, 6))
sns.histplot(df_switch_pd['transaction_count'], bins=30, kde=True)
plt.title("Distribution of Transaction Count")
plt.xlabel("Transaction Count")
plt.ylabel("Frequency")
plt.show()

# Analyzing recent activity
plt.figure(figsize=(10, 6))
sns.histplot(df_switch_pd['recent_activity_1m'], bins=30, kde=True)
plt.title("Distribution of Recent Activity (Last Month)")
plt.xlabel("Number of Transactions in Last Month")
plt.ylabel("Frequency")
plt.show()

# Geographic features analysis
state_count_df = df_switch_pd['state_count'].value_counts().reset_index()
state_count_df.columns = ['state_count', 'frequency']
plt.figure(figsize=(10, 6))
sns.barplot(data=state_count_df, x='state_count', y='frequency')
plt.title("Distribution of State Count")
plt.xlabel("Number of States")
plt.ylabel("Frequency")
plt.show()

country_count_df = df_switch_pd['country_count'].value_counts().reset_index()
country_count_df.columns = ['country_count', 'frequency']
plt.figure(figsize=(10, 6))
sns.barplot(data=country_count_df, x='country_count', y='frequency')
plt.title("Distribution of Country Count")
plt.xlabel("Number of Countries")
plt.ylabel("Frequency")
plt.show()

# Analyzing one-hot encoded product features
onehot_cols = [col for col in df_switch_pd.columns if 'standardized_prd_name_' in col]
onehot_summary = df_switch_pd[onehot_cols].sum().reset_index()
onehot_summary.columns = ['Product', 'Count']
onehot_summary = onehot_summary.sort_values(by='Count', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=onehot_summary, x='Count', y='Product')
plt.title("One-Hot Encoded Product Features")
plt.xlabel("Count")
plt.ylabel("Product")
plt.show()

# Correlation matrix
plt.figure(figsize=(12, 8))
correlation_matrix = df_switch_pd.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Summary of findings
summary = {
    "Account Longevity": df_switch_pd['account_longevity'].describe(),
    "Current Balance Amount": df_switch_pd['curr_bal_amt'].describe(),
    "Ledger Balance Amount": df_switch_pd['ledger_bal_amt'].describe(),
    "Product Diversity": df_switch_pd['product_diversity'].describe(),
    "Transaction Count": df_switch_pd['transaction_count'].describe(),
    "Recent Activity (Last Month)": df_switch_pd['recent_activity_1m'].describe(),
    "State Count": state_count_df,
    "Country Count": country_count_df,
    "Top Products Used": onehot_summary
}

for key, value in summary.items():
    print(f"{key}:\n{value}\n")

# Closing Spark session
spark.stop()




# Check if datediff calculation works in isolation
sample_dates = non_pwm.select("hh_id_in_wh", "business_date").groupBy("hh_id_in_wh").agg(
    min("business_date").alias("min_date"),
    spark_max("business_date").alias("max_date")
)

# Debug print: Check sample date calculation
sample_dates.show(5)

# Add datediff calculation to the sample
sample_dates = sample_dates.withColumn("non_pwm_duration", datediff(col("max_date"), col("min_date")))

# Debug print: Check datediff calculation
sample_dates.show(5)

# Perform the groupBy and aggregation step-by-step

# Step 1: Calculate the duration each consumer stayed non-PWM
non_pwm_duration = non_pwm.groupBy("hh_id_in_wh").agg(
    datediff(spark_max("business_date"), min("business_date")).alias("non_pwm_duration")
)

# Debug print: Check non-PWM duration calculation
non_pwm_duration.show(5)

# Step 2: Calculate the count of non-PWM records for each consumer
non_pwm_count = non_pwm.groupBy("hh_id_in_wh").agg(
    count("seg_code").alias("non_pwm_count")
)

# Debug print: Check non-PWM count calculation
non_pwm_count.show(5)

# Step 3: Get the last non-PWM date for each consumer
last_non_pwm_date = non_pwm.groupBy("hh_id_in_wh").agg(
    spark_max("business_date").alias("last_non_pwm_date")
)

# Debug print: Check last non-PWM date calculation
last_non_pwm_date.show(5)

# Combine the above results
non_pwm_consistent = non_pwm_duration.join(non_pwm_count, "hh_id_in_wh").join(last_non_pwm_date, "hh_id_in_wh")

# Ensure the consumer has been non-PWM for the full 6 months (180 days)
non_pwm_consistent = non_pwm_consistent.filter((col("non_pwm_duration") >= 180) & (col("non_pwm_count") >= 180))

# Debug print: Check the final non_pwm_consistent DataFrame
non_pwm_consistent.show(5)

# Identify consumers who switch to PWM after the non-PWM period
switch_to_pwm = df.filter(col("seg_code") == "PWM").select("hh_id_in_wh", "business_date").distinct()

# Debug print: Check the switch_to_pwm DataFrame
switch_to_pwm.show(5)

# Identify valid switches
valid_switches = non_pwm_consistent.join(switch_to_pwm, "hh_id_in_wh", "inner") \
    .filter(col("business_date") > col("last_non_pwm_date")).select("hh_id_in_wh").distinct()

# Debug print: Check the valid_switches DataFrame
valid_switches.show(5)

# Extract the necessary data for analysis
df_switch = df.join(valid_switches, "hh_id_in_wh", "inner")
df_switch_pd = df_switch.toPandas()









# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql.functions import col

# Convert Spark DataFrame to Pandas DataFrame for EDA
df_switch_pd = df_switch.toPandas()

# Overview of the data
print("Data Overview:")
print(df_switch_pd.info())
print(df_switch_pd.describe())

# Plotting distribution of current and ledger balance amounts
if 'curr_bal_amt' in df_switch_pd.columns and 'ledger_bal_amt' in df_switch_pd.columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df_switch_pd[['curr_bal_amt', 'ledger_bal_amt']])
    plt.title("Boxplot of Current Balance and Ledger Balance Amounts")
    plt.ylabel("Balance Amount")
    plt.show()

# Analyzing geographical distribution by state
if 'st_code' in df_switch_pd.columns:
    state_count_df = df_switch_pd['st_code'].value_counts().reset_index()
    state_count_df.columns = ['st_code', 'frequency']
    plt.figure(figsize=(10, 6))
    sns.barplot(data=state_count_df, x='st_code', y='frequency')
    plt.title("Distribution of State Code")
    plt.xlabel("State Code")
    plt.ylabel("Frequency")
    plt.show()

# Analyzing geographical distribution by country
if 'cntry_code' in df_switch_pd.columns:
    country_count_df = df_switch_pd['cntry_code'].value_counts().reset_index()
    country_count_df.columns = ['cntry_code', 'frequency']
    plt.figure(figsize=(10, 6))
    sns.barplot(data=country_count_df, x='cntry_code', y='frequency')
    plt.title("Distribution of Country Code")
    plt.xlabel("Country Code")
    plt.ylabel("Frequency")
    plt.show()

# Analyzing product code distribution
if 'prd_code' in df_switch_pd.columns:
    product_count_df = df_switch_pd['prd_code'].value_counts().reset_index()
    product_count_df.columns = ['prd_code', 'frequency']
    plt.figure(figsize=(10, 6))
    sns.barplot(data=product_count_df, x='prd_code', y='frequency')
    plt.title("Distribution of Product Codes")
    plt.xlabel("Product Code")
    plt.ylabel("Frequency")
    plt.show()

# Analyzing primary officer code distribution
if 'prim_officer_code' in df_switch_pd.columns:
    officer_count_df = df_switch_pd['prim_officer_code'].value_counts().reset_index()
    officer_count_df.columns = ['prim_officer_code', 'frequency']
    plt.figure(figsize=(10, 6))
    sns.barplot(data=officer_count_df, x='prim_officer_code', y='frequency')
    plt.title("Distribution of Primary Officer Codes")
    plt.xlabel("Primary Officer Code")
    plt.ylabel("Frequency")
    plt.show()

# Correlation matrix for numerical features
numerical_features = ['curr_bal_amt', 'ledger_bal_amt']
plt.figure(figsize=(12, 8))
correlation_matrix = df_switch_pd[numerical_features].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Summary of findings
summary = {
    "Current Balance Amount": df_switch_pd['curr_bal_amt'].describe() if 'curr_bal_amt' in df_switch_pd.columns else "N/A",
    "Ledger Balance Amount": df_switch_pd['ledger_bal_amt'].describe() if 'ledger_bal_amt' in df_switch_pd.columns else "N/A",
    "State Count": state_count_df if 'st_code' in df_switch_pd.columns else "N/A",
    "Country Count": country_count_df if 'cntry_code' in df_switch_pd.columns else "N/A",
    "Product Code Distribution": product_count_df if 'prd_code' in df_switch_pd.columns else "N/A",
    "Primary Officer Code Distribution": officer_count_df if 'prim_officer_code' in df_switch_pd.columns else "N/A"
}

for key, value in summary.items():
    print(f"{key}:\n{value}\n")





import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming current_balance_pd is your DataFrame
# Summing the curr_bal_amt values for each hh_id_in_wh and retaining the group information
summed_balance_df = current_balance_pd.groupby(['hh_id_in_wh', 'group'], as_index=False)['curr_bal_amt'].sum()

# Plotting the distribution of summed current balance amounts for each group separately on the same image
plt.figure(figsize=(10, 6))

groups = summed_balance_df['group'].unique()
for group in groups:
    sns.kdeplot(data=summed_balance_df[summed_balance_df['group'] == group], x='curr_bal_amt', label=group)

plt.title("Current Balance Distribution Comparison")
plt.xlabel("Current Balance Amount")
plt.ylabel("Density")
plt.legend(title='Group')
plt.show()


from pyspark.sql.functions import avg, date_sub

# Filter data for the last three months
max_date = combined_df.agg(max("business_date")).collect()[0][0]
three_months_ago = date_sub(max_date, 90)

last_three_months_df = combined_df.filter(col("business_date") >= three_months_ago)

# Calculate the average balance for the last three months for each client
last_three_months_avg_df = last_three_months_df.groupBy("hh_id_in_wh", "group") \
                                               .agg(avg("curr_bal_amt").alias("avg_last_three_months_bal"))

# Convert to Pandas for visualization
last_three_months_avg_pd = last_three_months_avg_df.toPandas()

plt.figure(figsize=(10, 6))
sns.histplot(data=last_three_months_avg_pd, x="avg_last_three_months_bal", hue="group", kde=True)
plt.title("Last Three Months Average Balance Distribution by Group")
plt.xlabel("Average Balance Amount")
plt.ylabel("Frequency")
plt.show()




import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'product_type_pd' is the DataFrame you have after concatenation
# Example DataFrame creation (adjust this part to your actual DataFrame)
product_type_pd = pd.DataFrame({
    'prd_code': [...],  # your product codes
    'group': [...]      # your groups
})

# Aggregating the count of product types by group
product_count = product_type_pd.groupby(['prd_code', 'group']).size().reset_index(name='count')

# Plotting the data
plt.figure(figsize=(12, 8))
sns.barplot(data=product_count, x='prd_code', y='count', hue='group')
plt.title('Product Type Distribution by Group')
plt.xlabel('Product Code')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.tight_layout()

# Save the plot
plt.savefig('/mnt/data/product_type_distribution.png')

# Display the plot
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt

# Ensure the 'group' column is treated as a categorical variable
filtered_balance['group'] = filtered_balance['group'].astype('category')

# Create a distribution plot
plt.figure(figsize=(12, 8))

# Plot the distribution for each group separately
for group in filtered_balance['group'].cat.categories:
    sns.kdeplot(data=filtered_balance[filtered_balance['group'] == group], 
                x='curr_bal_amt', 
                fill=True, 
                alpha=0.5, 
                label=group)

plt.title('Distribution of curr_bal_amt by Group')
plt.xlabel('Current Balance Amount')
plt.ylabel('Density')
plt.legend(title='Group')
plt.show()


