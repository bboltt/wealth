Definition: A valid conversion occurs when a consumer, who does not have any PWM relationship at the time the model is run, is later assigned to a PWM relationship (i.e., becomes a PWM client). Our model's performance is measured by how many of these valid conversions were correctly identified by the model.

Example:

Hank Meeks is a consumer without a PWM relationship as of September 30, 2023.
The prospecting model runs using data up to this date and identifies Hank Meeks as a high-potential prospect.
On April 30, 2024, Hank Meeks becomes a PWM client and is assigned to a relationship.
This transition is considered a valid conversion, and since it was predicted by the model, it counts as a successful identification.













Here's a detailed description of the hyperparameter tuning process for your DeepFM model, covering the parameters you've adjusted:

---

In the development of our DeepFM model, we undertook a comprehensive hyperparameter tuning process to optimize model performance. Hereâ€™s an outline of the key hyperparameters adjusted:

1. **Embedding Dimension (`embedding_dim`)**: This parameter controls the size of the embedding vectors for the categorical features. A larger embedding dimension can capture more detailed interactions between features but may lead to overfitting and increased computational cost.

2. **Zero Ratio (`zero_ratio`)**: Given the imbalance in our dataset, where instances with label=0 (no interaction) vastly outnumber those with label=1 (interaction), `zero_ratio` specifies the desired ratio of label=0 to label=1 samples in our training set. This hyperparameter helps in balancing the data by undersampling the majority class, thus enhancing the model's ability to generalize rather than memorize the majority class.

3. **Minimum Zero Count (`atleast`)**: This parameter sets a threshold for the minimum count of label=0 samples. It ensures that, regardless of the `zero_ratio`, the count of the label=0 samples is always at least equal to this threshold, providing a baseline level of representation for the majority class in the training data. This is particularly useful in scenarios where the `zero_ratio` does not provide enough majority samples due to large fluctuations in the dataset size across different training phases.

4. **L2 Regularization (`l2_reg`)**: L2 regularization was applied to prevent the model from overfitting by penalizing large weights. This adds a penalty term to the loss function proportional to the sum of the squared weights, thus encouraging the model to maintain smaller weights and, consequently, a simpler model.

5. **Dropout Rate (`dropout_rate`)**: To further mitigate the risk of overfitting, a dropout rate was specified. This rate determines the fraction of neurons to randomly drop out during training, forcing the model to learn more robust features that are useful in conjunction with multiple different random subsets of the other neurons.

The tuning of these hyperparameters was carried out using a grid search strategy, assessing the model's performance via cross-validation on the training data. Adjustments were made iteratively to hone in on the optimal settings that maximize the validation set's performance, particularly focusing on metrics such as AUC and F1-score, which are critical for imbalanced datasets like ours.

This structured approach to hyperparameter tuning has been essential in enhancing the predictive accuracy and generalizability of our DeepFM model, making it adept at handling the nuances of our imbalanced dataset.



Cluster 0:
Balance Trend: Extremely high positive balance trend of 969,500, the highest among all clusters.
Product Holdings:
Money Market: 53.82% ownership, which is much higher compared to other clusters.
HELOC: Notably high proportion of households with HELOC accounts.
Cluster 1:
Balance Trend: Only cluster with a negative balance trend (-10,131), which is a strong differentiator.
Product Holdings:
Commercial Line of Credit: High presence in this cluster.
HELOC: High ownership compared to other clusters.
Roth IRA: Significant representation of Roth IRA accounts.
Cluster 2:
Recent Openings: Most active cluster in the last 30 days, with 3 new products opened.
Balance Trend: Positive trend of 428,006.
Product Holdings:
Private Wealth CD: High ownership.
Individual IRA: Prominent in this cluster.
Cluster 3:
Balance Trend: Moderate positive trend of 379,054, lower than Cluster 0 but still notable.
Product Holdings:
Trust Accounts: Significant proportion of Trust and Roth IRA accounts.
Private Wealth CD: High representation in this cluster.
Cluster 4:
Balances: Lower current and ledger balances, reflecting more conservative financial profiles.
Product Holdings:
Trust Accounts: Strong representation of Trust products.
Cluster 5:
Recent Product Openings: High activity in opening credit products like Credit Cards and Loans.
Product Holdings:
Prime Time Checking: Higher ownership compared to other clusters.
Cluster 6:
Balances: Very low balances and balance trends compared to other clusters, indicating low financial activity.
Product Holdings:
SEP IRA and Traditional IRA: Higher ownership, showing a focus on retirement planning.
Cluster 7:
Balance Trend: Positive trend of 262,308, slightly above average but lower than Clusters 0 and 2.
Product Holdings:
Revocable Trust: High ownership of Revocable Trusts compared to other clusters.
Private Wealth CD: Strong presence, similar to Cluster 2.
Cluster 8:
Balance Trend: Moderate positive balance trend of 105,297.
Product Holdings:
Unsecured Line: High ownership of Unsecured Line products, not seen much in other clusters.
Cluster 9:
Balances: Very low balance trend (9,097) and low financial activity.
Product Holdings:
Roth IRA and Money Market: Higher ownership in these categories, despite low overall financial activity.
General Observations:
Cluster 0 stands out with the highest positive balance trend and high ownership of Money Market and HELOC products.
Cluster 1 is distinct with a negative balance trend and high ownership of Commercial Line of Credit and Roth IRA products.
Clusters 2 and 7 show significant Private Wealth CD ownership, with Cluster 2 also being highly active in recent product openings.
Cluster 6 is notable for its focus on retirement products like SEP IRA and Traditional IRA.
Cluster 9 has low balance trends but stands out for ownership of Roth IRA and Money Market products.























import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
import numpy as np
import pandas as pd




# Apply PCA to reduce to 3 dimensions
pca_3d = PCA(k=3, inputCol="scaledFeatures", outputCol="pcaFeatures")
pca_model_3d = pca_3d.fit(df_with_clusters)
df_pca_3d = pca_model_3d.transform(df_with_clusters)

# Convert to Pandas for 3D plotting
df_pca_3d_pd = df_pca_3d.select("pcaFeatures", "prediction").toPandas()
df_pca_3d_pd['PCA1'] = df_pca_3d_pd['pcaFeatures'].apply(lambda x: x[0])
df_pca_3d_pd['PCA2'] = df_pca_3d_pd['pcaFeatures'].apply(lambda x: x[1])
df_pca_3d_pd['PCA3'] = df_pca_3d_pd['pcaFeatures'].apply(lambda x: x[2])

# Assuming df_pca_3d_pd is your DataFrame with the 3D PCA features and cluster labels
# Calculate the global x, y, and z limits
x_min = df_pca_3d_pd['PCA1'].min()
x_max = df_pca_3d_pd['PCA1'].max()
y_min = df_pca_3d_pd['PCA2'].min()
y_max = df_pca_3d_pd['PCA2'].max()
z_min = df_pca_3d_pd['PCA3'].min()
z_max = df_pca_3d_pd['PCA3'].max()

# Number of clusters
num_clusters = df_pca_3d_pd['prediction'].nunique()

# Create 3D subplots (adjust rows/cols for number of clusters)
fig = plt.figure(figsize=(14, 12))
axes = []

# Create subplots for each cluster
for i, cluster in enumerate(df_pca_3d_pd['prediction'].unique()):
    ax = fig.add_subplot(num_clusters // 2 + num_clusters % 2, 2, i + 1, projection='3d')
    subset = df_pca_3d_pd[df_pca_3d_pd['prediction'] == cluster]
    
    # Scatter plot for each cluster
    ax.scatter(subset['PCA1'], subset['PCA2'], subset['PCA3'], s=100, alpha=0.7)
    
    # Set title and labels
    ax.set_title(f'Cluster {cluster}')
    ax.set_xlabel('PCA1')
    ax.set_ylabel('PCA2')
    ax.set_zlabel('PCA3')
    
    # Set the same limits for all axes
    ax.set_xlim(x_min, x_max)
    ax.set_ylim(y_min, y_max)
    ax.set_zlim(z_min, z_max)

    axes.append(ax)

# Adjust layout
plt.tight_layout()
plt.show()



from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
import pandas as pd
import numpy as np

# Assuming df_with_clusters is your DataFrame with the features and cluster labels
df_pandas = df_with_clusters.select("scaledFeatures", "prediction").toPandas()

# Extract features and labels
X = np.array(df_pandas["scaledFeatures"].values.tolist())  # Convert the 'scaledFeatures' column to a 2D array
y = df_pandas['prediction']

# Apply t-SNE for dimensionality reduction to 3D
tsne = TSNE(n_components=3, perplexity=30, n_iter=300)
X_tsne = tsne.fit_transform(X)

# Create a DataFrame for plotting
df_tsne = pd.DataFrame(X_tsne, columns=['t-SNE1', 't-SNE2', 't-SNE3'])
df_tsne['cluster'] = y

# Create a 3D plot
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Create scatter plot
scatter = ax.scatter(df_tsne['t-SNE1'], df_tsne['t-SNE2'], df_tsne['t-SNE3'], 
                     c=df_tsne['cluster'], cmap='Set1', s=100, alpha=0.7)

# Set labels
ax.set_xlabel('t-SNE1')
ax.set_ylabel('t-SNE2')
ax.set_zlabel('t-SNE3')
plt.title("3D t-SNE Cluster Visualization")

# Add a legend
plt.legend(*scatter.legend_elements(), title="Clusters")

# Show the plot
plt.show()

































from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming df_with_clusters is your DataFrame with the features and cluster labels
df_pandas = df_with_clusters.select("scaledFeatures", "prediction").toPandas()

# Extract features and labels
# scaledFeatures contains a list for each row, so you need to convert it to a 2D NumPy array
X = np.array(df_pandas["scaledFeatures"].values.tolist())  # Convert the 'scaledFeatures' column to a 2D array
y = df_pandas['prediction']

# Apply t-SNE for dimensionality reduction to 2D
tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
X_tsne = tsne.fit_transform(X)

# Create a DataFrame for plotting
df_tsne = pd.DataFrame(X_tsne, columns=['t-SNE1', 't-SNE2'])
df_tsne['cluster'] = y

# Plot t-SNE result
plt.figure(figsize=(10, 7))
sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='cluster', palette='Set1', data=df_tsne, s=100, alpha=0.7)
plt.title("t-SNE Cluster Visualization")
plt.show()


import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Calculate the global x and y limits
x_min = df_pca_pd['PCA1'].min()
x_max = df_pca_pd['PCA1'].max()
y_min = df_pca_pd['PCA2'].min()
y_max = df_pca_pd['PCA2'].max()

# Number of clusters
num_clusters = df_pca_pd['prediction'].nunique()

# Create subplots (rows x cols depending on the number of clusters)
fig, axes = plt.subplots(nrows=num_clusters // 2 + num_clusters % 2, ncols=2, figsize=(14, 12))  # Adjust rows/cols for number of clusters
axes = axes.flatten()  # Flatten the axes array to iterate

# Plot each cluster separately
for i, cluster in enumerate(df_pca_pd['prediction'].unique()):
    subset = df_pca_pd[df_pca_pd['prediction'] == cluster]
    ax = axes[i]  # Get the corresponding axis
    sns.scatterplot(x='PCA1', y='PCA2', data=subset, s=100, alpha=0.7, ax=ax)
    ax.set_title(f'Cluster {cluster}')
    
    # Set the same x and y limits for each plot
    ax.set_xlim(x_min, x_max)
    ax.set_ylim(y_min, y_max)

# Remove any empty subplots if the number of clusters is odd
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout
plt.tight_layout()
plt.show()





plt.figure(figsize=(10, 7))
sns.scatterplot(x='PCA1', y='PCA2', hue='prediction', palette='Set1', data=df_pca_pd, s=200, alpha=0.5)
plt.title("2D PCA Cluster Visualization with Transparency")
plt.show()

plt.figure(figsize=(14, 12))
for cluster in df_pca_pd['prediction'].unique():
    subset = df_pca_pd[df_pca_pd['prediction'] == cluster]
    plt.scatter(subset['PCA1'], subset['PCA2'], label=f'Cluster {cluster}', s=100, alpha=0.7)

plt.title("Clusters Visualized Separately")
plt.legend()
plt.show()



import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Number of clusters
num_clusters = df_pca_pd['prediction'].nunique()

# Create subplots (rows x cols depending on the number of clusters)
fig, axes = plt.subplots(nrows=num_clusters // 2, ncols=2, figsize=(14, 12))  # Adjust rows/cols for number of clusters
axes = axes.flatten()  # Flatten the axes array to iterate

# Plot each cluster separately
for i, cluster in enumerate(df_pca_pd['prediction'].unique()):
    subset = df_pca_pd[df_pca_pd['prediction'] == cluster]
    ax = axes[i]  # Get the corresponding axis
    sns.scatterplot(x='PCA1', y='PCA2', data=subset, s=100, alpha=0.7, ax=ax)
    ax.set_title(f'Cluster {cluster}')

# Adjust layout
plt.tight_layout()
plt.show()






def perform_clustering(spark, df, features, k):
    """
    Performs K-Means clustering on the PWM data and returns the cluster centers and data with cluster assignments.
    
    Args:
        df (DataFrame): DataFrame containing only PWM client data.
        features (list): List of feature names to include in the clustering.
        k (int): Number of clusters.
        
    Returns:
        centers_df (DataFrame): DataFrame containing cluster centers.
        df_with_clusters (DataFrame): DataFrame with cluster assignments.
    """
    assembler = VectorAssembler(inputCols=features, outputCol="features", handleInvalid="skip")
    scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")
    
    # Pipeline: Assemble features -> Normalize -> Cluster
    kmeans = KMeans(featuresCol="scaledFeatures", k=k, seed=42)
    pipeline = Pipeline(stages=[assembler, scaler, kmeans])
    
    model = pipeline.fit(df)
    
    # Cluster centers
    centers = model.stages[-1].clusterCenters()
    centers_rows = [Row(features=center.tolist()) for center in centers]
    centers_df = spark.createDataFrame(centers_rows)
    
    # Data with cluster assignments
    df_with_clusters = model.transform(df).select("hh_id_in_wh", "scaledFeatures", "prediction")
    
    return centers_df, df_with_clusters


from pyspark.ml.feature import PCA
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Perform clustering and get the data with cluster assignments
centers_df, df_with_clusters = perform_clustering(spark, df, features, k=5)

# Reduce dimensions with PCA (from Spark)
pca = PCA(k=2, inputCol="scaledFeatures", outputCol="pcaFeatures")
pca_model = pca.fit(df_with_clusters)
df_pca = pca_model.transform(df_with_clusters)

# Convert the result to Pandas for plotting
df_pca_pd = df_pca.select("pcaFeatures", "prediction").toPandas()

# Create separate columns for PCA1 and PCA2
df_pca_pd['PCA1'] = df_pca_pd['pcaFeatures'].apply(lambda x: x[0])
df_pca_pd['PCA2'] = df_pca_pd['pcaFeatures'].apply(lambda x: x[1])

# Plot the clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(x='PCA1', y='PCA2', hue='prediction', palette='Set1', data=df_pca_pd, s=100)
plt.title("2D Cluster Visualization (PCA Reduced)")
plt.show()


from mpl_toolkits.mplot3d import Axes3D

# Apply PCA to reduce to 3 dimensions
pca_3d = PCA(k=3, inputCol="scaledFeatures", outputCol="pcaFeatures")
pca_model_3d = pca_3d.fit(df_with_clusters)
df_pca_3d = pca_model_3d.transform(df_with_clusters)

# Convert to Pandas for 3D plotting
df_pca_3d_pd = df_pca_3d.select("pcaFeatures", "prediction").toPandas()
df_pca_3d_pd['PCA1'] = df_pca_3d_pd['pcaFeatures'].apply(lambda x: x[0])
df_pca_3d_pd['PCA2'] = df_pca_3d_pd['pcaFeatures'].apply(lambda x: x[1])
df_pca_3d_pd['PCA3'] = df_pca_3d_pd['pcaFeatures'].apply(lambda x: x[2])

# 3D Scatter plot
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(df_pca_3d_pd['PCA1'], df_pca_3d_pd['PCA2'], df_pca_3d_pd['PCA3'], 
                     c=df_pca_3d_pd['prediction'], cmap='Set1')

ax.set_xlabel('PCA1')
ax.set_ylabel('PCA2')
ax.set_zlabel('PCA3')
plt.title("3D Cluster Visualization")

plt.show()





