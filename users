from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, expr
from pyspark.sql.window import Window

def add_opened_counts(spark, df):
    # Convert business_date and open_date columns to date type if they are in string format
    df = df.withColumn("business_date", col("business_date").cast("date"))
    df = df.withColumn("open_date", col("open_date").cast("date"))

    # Define window specifications for each time range
    windowSpec = Window.partitionBy("Segmt_Prod_Type").orderBy("business_date").rangeBetween(Window.unboundedPreceding, Window.currentRow)

    # Calculate users who opened the item in the past 1 month
    df = df.withColumn("users_past_1_month", count(when(col("open_date") >= col("business_date") - expr("INTERVAL 1 MONTH"), True)).over(windowSpec))

    # Calculate users who opened the item in the past 2 months
    df = df.withColumn("users_past_2_months", count(when(col("open_date") >= col("business_date") - expr("INTERVAL 2 MONTHS"), True)).over(windowSpec))

    # Calculate users who opened the item in the past 6 months
    df = df.withColumn("users_past_6_months", count(when(col("open_date") >= col("business_date") - expr("INTERVAL 6 MONTHS"), True)).over(windowSpec))

    # Calculate users who opened the item in the past 12 months
    df = df.withColumn("users_past_12_months", count(when(col("open_date") >= col("business_date") - expr("INTERVAL 12 MONTHS"), True)).over(windowSpec))

    return df

# Example usage
spark = SparkSession.builder.appName("RecommenderSystem").getOrCreate()

data = [
    (1, 'A', '2023-01-01', '2022-01-01', '2021-01-01', 1),
    (2, 'A', '2023-01-02', '2022-01-02', '2021-01-02', 1),
    (3, 'A', '2023-01-01', '2022-01-03', '2021-01-03', 1),
    (1, 'B', '2023-01-01', '2022-01-01', '2021-01-01', 1),
    (2, 'B', '2023-01-03', '2022-01-03', '2021-01-03', 1)
]

columns = ['ip_id', 'Segmt_Prod_Type', 'business_date', 'open_date', 'one_year_before_open_date', 'Flag']

df = spark.createDataFrame(data, columns)

# Add the counts columns
df_with_counts = add_opened_counts(spark, df)

# Show the resulting DataFrame
df_with_counts.show()
