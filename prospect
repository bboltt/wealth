from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lag, lit, when
from pyspark.sql.window import Window

# Initialize Spark session
spark = SparkSession.builder.appName("GroundTruthExample").getOrCreate()

# Example DataFrame with relevant columns
data = [
    (1, "PWM", "2023-01-01"),
    (1, "NON_PWM", "2023-02-01"),
    (1, "PWM", "2023-03-01"),
    (2, "NON_PWM", "2023-01-01"),
    (2, "NON_PWM", "2023-02-01"),
    (2, "PWM", "2023-03-01"),
    (3, "PWM", "2023-01-01"),
    (3, "PWM", "2023-02-01"),
    (3, "PWM", "2023-03-01"),
    (4, "NON_PWM", "2023-01-01"),
    (4, "NON_PWM", "2023-02-01"),
    (4, "NON_PWM", "2023-03-01")
]

columns = ["hh_id_in_wh", "seg_code", "business_date"]

df = spark.createDataFrame(data, columns)

# Reference date and target period
reference_date = "2023-01-01"
target_period_start = reference_date
target_period_end = "2023-07-01"  # 6 months after the reference date

# Filter data for the target period
df = df.filter((col("business_date") >= lit(target_period_start)) & (col("business_date") < lit(target_period_end)))

# Window specification to look at the previous segment code
window_spec = Window.partitionBy("hh_id_in_wh").orderBy("business_date")

# Identify switches from NON_PWM to PWM
df = df.withColumn("prev_seg_code", lag("seg_code").over(window_spec))
df = df.withColumn("switched_to_pwm", when((col("prev_seg_code") != "PWM") & (col("seg_code") == "PWM"), 1).otherwise(0))

# Select distinct hh_id_in_wh that switched to PWM
ground_truth = df.filter(col("switched_to_pwm") == 1).select("hh_id_in_wh").distinct()

# Show the ground truth
ground_truth.show()

# Stop Spark session
spark.stop()



# Assuming ground_truth and top_k_consumers DataFrames have been defined
ground_truth_set = set(row.hh_id_in_wh for row in ground_truth.collect())
top_k_consumers_set = set(row.hh_id_in_wh for row in top_k_consumers.collect())

# Calculate True Positives, False Positives, and False Negatives
true_positives = ground_truth_set.intersection(top_k_consumers_set)
false_positives = top_k_consumers_set - ground_truth_set
false_negatives = ground_truth_set - top_k_consumers_set

# Calculate Precision and Recall
precision = len(true_positives) / (len(true_positives) + len(false_positives)) if (len(true_positives) + len(false_positives)) > 0 else 0
recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if (len(true_positives) + len(false_negatives)) > 0 else 0

print(f"Precision: {precision}")
print(f"Recall: {recall}")
















from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when, countDistinct, datediff, max as spark_max, min as spark_min, lag, avg, stddev, sum as spark_sum, variance, count
from pyspark.sql.window import Window
from pyspark.sql.types import IntegerType
from pyspark.ml.feature import StringIndexer
import pandas as pd

class FeatureEngineeringPipeline:
    """
    This is the main feature engineering class.
    
    Usage:
    Assuming dm_r3.pwm_mstr_dtl_daily data is loaded into a DataFrame df.
    pipeline = FeatureEngineeringPipeline(df)
    df_enriched = pipeline.execute()
    
    """
    def __init__(self, df, reference_date=None, doc_path=None):
        self.doc_path = doc_path
        if not reference_date:
            self.reference_date = current_date()
        else:
            self.reference_date = to_date(lit(reference_date), "yyyy-MM-dd")
        self.df = df.filter(col("business_date") <= self.reference_date)
        self.hh_features = self.df.select("hh_id_in_wh").distinct()
    
    def standardize_product_name(self, spark):
        if not self.doc_path:
            print("Product name standardization failed: product mapping file path is not defined")
            return
          
        # read mapping file
        mapping_pd = pd.read_excel(self.doc_path)
        mapping_df = spark.createDataFrame(mapping_pd)
        mapping_df = (mapping_df
                      .withColumnRenamed('Detail Product Name','prd_name')
                      .withColumnRenamed('Potential Target Product Name ','standardized_prd_name')
                     )
        self.df = (self.df
                     .join(
                       mapping_df.select('prd_name','standardized_prd_name'),
                       ('prd_name'),
                       'left'
                          )
                      .fillna({'standardized_prd_name': 'Other'})
                    )
        
    def convert_dates(self):
        # Convert dates from string to date type
        self.df = self.df.withColumn("business_date", col("business_date").cast("date"))
        self.df = self.df.withColumn("open_date", col("open_date").cast("date"))
        self.df = self.df.withColumn("close_date", col("close_date").cast("date"))
      
    def add_household_longevity(self):
        first_open_date = self.df.groupBy("hh_id_in_wh").agg(spark_min("open_date").alias("first_open_date"))
        longevity = first_open_date.withColumn("hh_longevity", datediff(self.reference_date, col("first_open_date")))
        self.hh_features = self.hh_features.join(longevity.select("hh_longevity", "hh_id_in_wh"), "hh_id_in_wh", "left").fillna(0)
    
    def add_balance_features(self):
        # Calculate mean, max, and min balances (curr_bal_amt, ledger_bal_amt)
        self.df = self.df.withColumn("curr_bal_amt_float", col("curr_bal_amt").cast("float"))
        self.df = self.df.withColumn("ledger_bal_amt_float", col("ledger_bal_amt").cast("float"))
        windowSpec = Window.partitionBy("hh_id_in_wh").orderBy(col("business_date").desc())
        row_number = self.df.withColumn("row_num", F.row_number().over(windowSpec))
        curr_bal_amt = row_number.filter(col("row_num") == 1).select("hh_id_in_wh", "curr_bal_amt_float", "ledger_bal_amt_float")
        curr_bal_amt = curr_bal_amt.withColumnRenamed("curr_bal_amt_float", "curr_bal_amt")
        curr_bal_amt = curr_bal_amt.withColumnRenamed("ledger_bal_amt_float", "curr_ledger_bal_amt")

        balance_stats = self.df.groupBy("hh_id_in_wh").agg(
            avg("curr_bal_amt_float").alias("bal_amt_mean"),
            spark_max("curr_bal_amt_float").alias("bal_amt_max"),
            spark_min("curr_bal_amt_float").alias("bal_amt_min"),
            stddev("curr_bal_amt_float").alias("bal_amt_std"),
            avg("ledger_bal_amt_float").alias("ledger_bal_amt_mean"),
            stddev("ledger_bal_amt_float").alias("ledger_bal_amt_std"),
            spark_max("ledger_bal_amt_float").alias("ledger_bal_amt_max"),
            spark_min("ledger_bal_amt_float").alias("ledger_bal_amt_min")
        )
        
        self.hh_features = self.hh_features.join(curr_bal_amt, "hh_id_in_wh", "left").fillna(0)
        self.hh_features = self.hh_features.join(balance_stats, "hh_id_in_wh", "left").fillna(0)
        
    def add_temporal_features(self):
        self.df = self.df.withColumn("month", F.month("open_date"))
        self.df = self.df.withColumn("day_of_week", F.dayofweek("open_date"))
        self.df = self.df.withColumn("year", F.year("open_date"))
        return self
      
    def encode_categorical_data(self):
        indexer = StringIndexer(inputCol="prim_officer_code", outputCol="prim_officer_code_index")
        self.df = indexer.fit(self.df).transform(self.df)
        return self
    
    def add_product_diversity(self):
        # Count distinct products used by hh_id_in_wh
        product_diversity = self.df.groupBy("hh_id_in_wh").agg(
            countDistinct("standardized_prd_name").alias("product_diversity")
        )
        self.hh_features = self.hh_features.join(product_diversity, "hh_id_in_wh", "left").fillna(0)
    
    def add_transaction_count(self):
        # Count the number of transactions per household
        transaction_count = self.df.groupBy("hh_id_in_wh").agg(
            count("*").alias("transaction_count")
        )
        self.hh_features = self.hh_features.join(transaction_count, "hh_id_in_wh", "left").fillna(0)

    def add_transaction_recency(self):
        # Calculate days since the last transaction up to the reference date
        last_transaction_date = self.df.groupBy("hh_id_in_wh").agg(
            spark_max("business_date").alias("last_transaction_date")
        )
        transaction_recency = last_transaction_date.withColumn(
            "transaction_recency", datediff(self.reference_date, col("last_transaction_date"))
        )
        self.hh_features = self.hh_features.join(transaction_recency.select("transaction_recency", "hh_id_in_wh"), "hh_id_in_wh", "left").fillna(0)

    def add_geographic_features(self):
        # Count distinct states and countries for each household
        state_count = self.df.groupBy("hh_id_in_wh").agg(
            countDistinct("st_code").alias("state_count")
        )
        country_count = self.df.groupBy("hh_id_in_wh").agg(
            countDistinct("cntry_code").alias("country_count")
        )
        self.hh_features = self.hh_features.join(state_count, "hh_id_in_wh", "left").fillna(0)
        self.hh_features = self.hh_features.join(country_count, "hh_id_in_wh", "left").fillna(0)

    def add_account_type_count(self):
        # Count distinct account types for each household
        account_type_count = self.df.groupBy("hh_id_in_wh").agg(
            countDistinct("ar_type_code").alias("account_type_count")
        )
        self.hh_features = self.hh_features.join(account_type_count, "hh_id_in_wh", "left").fillna(0)

    def add_transaction_frequency(self):
        # Transaction frequency per month
        transaction_frequency = self.df.groupBy("hh_id_in_wh").agg(
            (count("*") / F.months_between(spark_max("business_date"), spark_min("business_date"))).alias("transaction_frequency")
        )
        self.hh_features = self.hh_features.join(transaction_frequency, "hh_id_in_wh", "left").fillna(0)

    def add_balance_trends(self):
        # Add trend in balances over time (linear regression slope as trend indicator)
        windowSpec = Window.partitionBy("hh_id_in_wh").orderBy(col("business_date"))
        df_with_lag = self.df.withColumn("prev_bal", lag("curr_bal_amt_float").over(windowSpec))
        df_with_trend = df_with_lag.withColumn("balance_trend", 
                                               (col("curr_bal_amt_float") - col("prev_bal")) / datediff(col("business_date"), lag("business_date").over(windowSpec)))
        balance_trend = df_with_trend.groupBy("hh_id_in_wh").agg(avg("balance_trend").alias("balance_trend"))
        self.hh_features = self.hh_features.join(balance_trend, "hh_id_in_wh", "left").fillna(0)

    def add_recent_activity(self):
        # Number of recent activities in the last month
        recent_activity = self.df.filter(col("business_date") >= date_add(self.reference_date, -30)).groupBy("hh_id_in_wh").agg(
            count("*").alias("recent_activity_1m")
        )
        self.hh_features = self.hh_features.join(recent_activity, "hh_id_in_wh", "left").fillna(0)

    def add_recent_product_diversity(self):
        # Diversity of products used in the last 3 months
        recent_product_diversity = self.df.filter(col("business_date") >= date_add(self.reference_date, -90)).groupBy("hh_id_in_wh").agg(
            countDistinct("standardized_prd_name").alias("recent_product_diversity_3m")
        )
        self.hh_features = self.hh_features.join(recent_product_diversity, "hh_id_in_wh", "left").fillna(0)
    
    def add_moving_averages(self):
        # Calculate moving averages of balances
        windowSpec = Window.partitionBy("hh_id_in_wh").orderBy(col("business_date")).rowsBetween(-2, 0)
        self.df = self.df.withColumn("moving_avg_bal", avg("curr_bal_amt_float").over(windowSpec))
        moving_avg_bal = self.df.groupBy("hh_id_in_wh").agg(spark_max("moving_avg_bal").alias("moving_avg_bal"))
        self.hh_features = self.hh_features.join(moving_avg_bal, "hh_id_in_wh", "left").fillna(0)

    def add_aggregated_statistics(self):
        # Aggregated statistics for balance and transaction metrics
        aggregated_stats = self.df.groupBy("hh_id_in_wh").agg(
            spark_sum("curr_bal_amt_float").alias("total_bal_amt"),
            variance("curr_bal_amt_float").alias("bal_amt_variance"),
            count("*").alias("transaction_count")
        )
        self.hh_features = self.hh_features.join(aggregated_stats, "hh_id_in_wh", "left").fillna(0)

    def add_rolling_statistics(self):
        # Rolling statistics for balance
        windowSpec = Window.partitionBy("hh_id_in_wh").orderBy(col("business_date")).rowsBetween(-2, 0)
        self.df = self.df.withColumn("rolling_std_bal", stddev("curr_bal_amt_float").over(windowSpec))
        rolling_std_bal = self.df.groupBy("hh_id_in_wh").agg(spark_max("rolling_std_bal").alias("rolling_std_bal"))
        self.hh_features = self.hh_features.join(rolling_std_bal, "hh_id_in_wh", "left").fillna(0)
    
    def add_onehot_encoded_products(self):
        # One-hot encode the standardized_prd_name for each hh_id_in_wh
        product_pivot = self.df.groupBy("hh_id_in_wh").pivot("standardized_prd_name").agg(count("standardized_prd_name")).fillna(0)
        
        # Convert counts to binary indicators
        for column in product_pivot.columns:
            if column != 'hh_id_in_wh':
                product_pivot = product_pivot.withColumn(column, (col(column) > 0).cast(IntegerType()))
        
        self.hh_features = self.hh_features.join(product_pivot, "hh_id_in_wh", "left").fillna(0)
    
    def filter_latest_data(self):
        latest_date = self.df.agg(spark_max("business_date")).collect()[0][0]
        self.df = self.df.filter(col("business_date") == latest_date)
    
    @timmer
    def execute(self, spark):
        self.standardize_product_name(spark)
        self.convert_dates()
        self.add_household_longevity()
        self.add_balance_features()
        self.add_product_diversity()
        self.add_transaction_count()
        self.add_transaction_recency()
        self.add_geographic_features()
        self.add_account_type_count()
        self.add_transaction_frequency()
        self.add_balance_trends()
        self.add_recent_activity()
        self.add_recent_product_diversity()
        self.add_moving_averages()
        self.add_aggregated_statistics()
        self.add_rolling_statistics()
        self.add_onehot_encoded_products()
        #self.add_temporal_features()
        #self.encode_categorical_data()
        # TODO: Include calls to new engineering methods
        return self.hh_features

