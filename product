from pyspark.sql.functions import countDistinct, sum, avg, expr, stddev, min, max, month, year


def filter_data_by_date(df, reference_date):
    """
    Filters the DataFrame to only include records on or before the given reference date.
    """
    return df.filter(col("open_date") <= lit(reference_date))


def count_users_per_product(df):
    return df.groupBy("prd_name").agg(
        countDistinct("ip_id").alias("total_users"),
        countDistinct(expr("CASE WHEN open_date IS NOT NULL THEN ip_id ELSE NULL END")).alias("users_opened"),
        countDistinct(expr("CASE WHEN close_date IS NOT NULL THEN ip_id ELSE NULL END")).alias("users_closed")
    )

def financial_metrics_per_product(df):
    return df.groupBy("prd_name").agg(
        sum("curr_bal_amt").alias("total_current_balance"),
        avg("curr_bal_amt").alias("avg_current_balance"),
        sum("ledger_bal_amt").alias("total_ledger_balance"),
        avg("ledger_bal_amt").alias("avg_ledger_balance")
    )


def average_open_duration_per_product(df):
    return df.withColumn("open_days", expr("DATEDIFF(close_date, open_date)")) \
        .groupBy("prd_name") \
        .agg(avg("open_days").alias("avg_open_duration_days"))

def advanced_financial_metrics(df):
    return df.groupBy("prd_name").agg(
        stddev("curr_bal_amt").alias("stddev_current_balance"),
        min("curr_bal_amt").alias("min_current_balance"),
        max("curr_bal_amt").alias("max_current_balance"),
        stddev("ledger_bal_amt").alias("stddev_ledger_balance"),
        min("ledger_bal_amt").alias("min_ledger_balance"),
        max("ledger_bal_amt").alias("max_ledger_balance")
    )

def temporal_features(df):
    # Add month and year columns to the DataFrame
    df = df.withColumn("month", month("open_date")) \
           .withColumn("year", year("open_date"))
    
    # Pivot on month and perform aggregations
    temp_df = df.groupBy("prd_name").pivot("month", list(range(1, 13))).agg(
        avg("curr_bal_amt").alias("avg_monthly_balance"),
        countDistinct("ip_id").alias("count_users_opened")
    )

    # Renaming columns to add 'month_' prefix and fill nulls with zeros
    for month_num in range(1, 13):
        avg_col = f"{month_num}_avg_monthly_balance"
        count_col = f"{month_num}_count_users_opened"
        temp_df = temp_df.withColumnRenamed(avg_col, f"month_{month_num}_avg_monthly_balance") \
                         .withColumnRenamed(count_col, f"month_{month_num}_count_users_opened") \
                         .na.fill({f"month_{month_num}_avg_monthly_balance": 0, f"month_{month_num}_count_users_opened": 0})

    return temp_df

def interaction_intensity_features(df):
    return df.groupBy("prd_name").agg(
        expr("COUNT(ip_id) / COUNT(DISTINCT ip_id)").alias("avg_interactions_per_user")
    )

def get_latest_product_status(df):
    """
    For each product, find the record with the latest open date.
    """
    # Determine the maximum open date for each product
    latest_dates = df.groupBy("prd_name").agg(max("open_date").alias("latest_date"))
    
    # Join this back to the original df to get the full row of the latest record for each product
    return df.join(latest_dates, (df["prd_name"] == latest_dates["prd_name"]) & (df["open_date"] == latest_dates["latest_date"])).drop(latest_dates["prd_name"])


def advanced_temporal_features(df):
    day_of_week_df = df.withColumn("day_of_week", dayofweek("open_date"))
    return day_of_week_df.groupBy("prd_name").pivot("day_of_week").agg(
        min("curr_bal_amt").alias("min_daily_balance"),
        max("curr_bal_amt").alias("max_daily_balance"),
        avg("curr_bal_amt").alias("avg_daily_balance"),
        stddev("curr_bal_amt").alias("std_daily_balance")
    ).select(
        "prd_name",
        *[col(c).alias(f"day_{c.split('_')[0]}_{c.split('_')[1]}_{c.split('_')[2]}") for c in day_of_week_df.columns if "balance" in c]
    )

def financial_variability_features(df):
    return df.groupBy("prd_name").agg(
        min("curr_bal_amt").alias("min_current_balance"),
        max("curr_bal_amt").alias("max_current_balance"),
        avg("curr_bal_amt").alias("avg_current_balance"),
        stddev("curr_bal_amt").alias("std_current_balance"),
        min("ledger_bal_amt").alias("min_ledger_balance"),
        max("ledger_bal_amt").alias("max_ledger_balance"),
        avg("ledger_bal_amt").alias("avg_ledger_balance"),
        stddev("ledger_bal_amt").alias("std_ledger_balance")
    )


def aggregate_product_features(spark, df, reference_date):
    df_filtered = filter_data_by_date(df, reference_date)
    df_latest_status = get_latest_product_status(df_filtered)

    # Generate features
    df_basic = count_users_per_product(df_latest_status)
    df_financial = financial_metrics_per_product(df_latest_status)
    df_advanced_financial = advanced_financial_metrics(df_latest_status)
    df_temporal = temporal_features(df_latest_status)
    df_advanced_temporal = advanced_temporal_features(df_latest_status)
    df_financial_variability = financial_variability_features(df_latest_status)
    df_average_duration = average_open_duration_per_product(df_latest_status)

    # Join all feature dataframes on 'prd_name', ensuring all possible columns are retained
    df_final = df_basic.join(df_financial, "prd_name", "outer") \
                       .join(df_advanced_financial, "prd_name", "outer") \
                       .join(df_temporal, "prd_name", "outer") \
                       .join(df_advanced_temporal, "prd_name", "outer") \
                       .join(df_financial_variability, "prd_name", "outer") \
                       .join(df_average_duration, "prd_name", "outer")


    return df_final


# Example usage
spark = generate_spark_instance(total_memory=300, total_vcpu=150)
df = spark.sql("select * from dm_r3.pwm_mstr_dtl_daily")
reference_date = '2023-12-31'
product_features_df = aggregate_product_features(spark, df, reference_date)
product_features_df.show()











