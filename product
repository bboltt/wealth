from pyspark.sql.functions import countDistinct, sum, avg, expr, stddev, min, max, month, year


def filter_data_by_date(df, reference_date):
    """
    Filters the DataFrame to only include records on or before the given reference date.
    """
    return df.filter(col("open_date") <= lit(reference_date))

def advanced_financial_metrics(df):
    return df.groupBy("prd_name").agg(
        stddev("curr_bal_amt").alias("stddev_current_balance"),
        min("curr_bal_amt").alias("min_current_balance"),
        max("curr_bal_amt").alias("max_current_balance"),
        stddev("ledger_bal_amt").alias("stddev_ledger_balance"),
        min("ledger_bal_amt").alias("min_ledger_balance"),
        max("ledger_bal_amt").alias("max_ledger_balance")
    )

def temporal_features(df):
    # Add month and year columns to the DataFrame
    df = df.withColumn("month", month("open_date")) \
           .withColumn("year", year("open_date"))
    
    # Pivot on month and perform aggregations
    temp_df = df.groupBy("prd_name").pivot("month", list(range(1, 13))).agg(
        avg("curr_bal_amt").alias("avg_monthly_balance"),
        countDistinct("ip_id").alias("count_users_opened")
    )

    # Renaming columns to add 'month_' prefix and fill nulls with zeros
    for month_num in range(1, 13):
        avg_col = f"{month_num}_avg_monthly_balance"
        count_col = f"{month_num}_count_users_opened"
        temp_df = temp_df.withColumnRenamed(avg_col, f"month_{month_num}_avg_monthly_balance") \
                         .withColumnRenamed(count_col, f"month_{month_num}_count_users_opened") \
                         .na.fill({f"month_{month_num}_avg_monthly_balance": 0, f"month_{month_num}_count_users_opened": 0})

    return temp_df

def interaction_intensity_features(df):
    return df.groupBy("prd_name").agg(
        expr("COUNT(ip_id) / COUNT(DISTINCT ip_id)").alias("avg_interactions_per_user")
    )

def get_latest_product_status(df):
    """
    For each product, find the record with the latest open date.
    """
    # Determine the maximum open date for each product
    latest_dates = df.groupBy("prd_name").agg(max("open_date").alias("latest_date"))
    
    # Join this back to the original df to get the full row of the latest record for each product
    return df.join(latest_dates, (df["prd_name"] == latest_dates["prd_name"]) & (df["open_date"] == latest_dates["latest_date"])).drop(latest_dates["prd_name"])


def advanced_temporal_features(df):
    df = df.withColumn("day_of_week", dayofweek("open_date"))
    return df.groupBy("prd_name").pivot("day_of_week").agg(
        avg("curr_bal_amt").alias("avg_daily_balance")
    )

def financial_variability_features(df):
    return df.groupBy("prd_name").agg(
        (stddev("curr_bal_amt") / mean("curr_bal_amt")).alias("coefficient_of_variation")
    )

def cross_product_interactions(df):
    # Assuming a column 'user_id' that uniquely identifies users
    user_products = df.groupBy("ip_id").agg(countDistinct("prd_name").alias("number_of_products"))
    more_than_one_product = user_products.filter(col("number_of_products") > 1).count()
    return df.withColumn("cross_product_users", lit(more_than_one_product))


def aggregate_product_features(spark, df, reference_date):
    df_filtered = filter_data_by_date(df, reference_date)
    df_latest_status = get_latest_product_status(df_filtered)

    df_basic = count_users_per_product(df_latest_status)
    df_financial = financial_metrics_per_product(df_latest_status)
    df_advanced_financial = advanced_financial_metrics(df_latest_status)
    df_temporal = temporal_features(df_latest_status)  # Original temporal features
    df_interaction = interaction_intensity_features(df_latest_status)
    df_average_duration = average_open_duration_per_product(df_latest_status)
    df_advanced_temporal = advanced_temporal_features(df_latest_status)  # Advanced temporal features
    df_financial_variability = financial_variability_features(df_latest_status)
    df_cross_product = cross_product_interactions(df_latest_status)

    # Join all feature dataframes on 'prd_name'
    df_final = df_basic.join(df_financial, "prd_name", "outer") \
                       .join(df_advanced_financial, "prd_name", "outer") \
                       .join(df_temporal, "prd_name", "outer") \
                       .join(df_advanced_temporal, "prd_name", "outer") \
                       .join(df_interaction, "prd_name", "outer") \
                       .join(df_average_duration, "prd_name", "outer") \
                       .join(df_financial_variability, "prd_name", "outer") \
                       .join(df_cross_product, "prd_name", "outer")

    return df_final




# Example usage
spark = generate_spark_instance(total_memory=300, total_vcpu=150)
df = spark.sql("select * from dm_r3.pwm_mstr_dtl_daily")
reference_date = '2023-12-31'
product_features_df = aggregate_product_features(spark, df, reference_date)
product_features_df.show()



