from pyspark.sql.functions import countDistinct, sum, avg, expr, stddev, min, max, month, year

def advanced_financial_metrics(df):
    return df.groupBy("prd_name").agg(
        stddev("curr_bal_amt").alias("stddev_current_balance"),
        min("curr_bal_amt").alias("min_current_balance"),
        max("curr_bal_amt").alias("max_current_balance"),
        stddev("ledger_bal_amt").alias("stddev_ledger_balance"),
        min("ledger_bal_amt").alias("min_ledger_balance"),
        max("ledger_bal_amt").alias("max_ledger_balance")
    )

def temporal_features(df):
    # Add month and year columns to the DataFrame
    df = df.withColumn("month", month("open_date")) \
           .withColumn("year", year("open_date"))
    
    # Pivot on month and perform aggregations
    temp_df = df.groupBy("prd_name").pivot("month", list(range(1, 13))).agg(
        avg("curr_bal_amt").alias("avg_monthly_balance"),
        countDistinct("ip_id").alias("count_users_opened")
    )

    # Renaming columns to add 'month_' prefix and fill nulls with zeros
    for month_num in range(1, 13):
        avg_col = f"{month_num}_avg_monthly_balance"
        count_col = f"{month_num}_count_users_opened"
        temp_df = temp_df.withColumnRenamed(avg_col, f"month_{month_num}_avg_monthly_balance") \
                         .withColumnRenamed(count_col, f"month_{month_num}_count_users_opened") \
                         .na.fill({f"month_{month_num}_avg_monthly_balance": 0, f"month_{month_num}_count_users_opened": 0})

    return temp_df

def interaction_intensity_features(df):
    return df.groupBy("prd_name").agg(
        expr("COUNT(ip_id) / COUNT(DISTINCT ip_id)").alias("avg_interactions_per_user")
    )

def aggregate_product_features(spark, df):
    df_basic = count_users_per_product(df)
    df_financial = financial_metrics_per_product(df)
    df_advanced_financial = advanced_financial_metrics(df)
    df_temporal = temporal_features(df)
    df_interaction = interaction_intensity_features(df)
    df_average_duration = average_open_duration_per_product(df)

    # Join all feature dataframes on 'prd_name'
    df_final = df_basic.join(df_financial, "prd_name", "outer") \
                       .join(df_advanced_financial, "prd_name", "outer") \
                       .join(df_temporal, "prd_name", "outer") \
                       .join(df_interaction, "prd_name", "outer") \
                       .join(df_average_duration, "prd_name", "outer")
    return df_final

# Example usage
spark = generate_spark_instance(total_memory=300, total_vcpu=150)
df = spark.sql("select * from dm_r3.pwm_mstr_dtl_daily")
product_features_df = aggregate_product_features(spark, df)
product_features_df.show()




