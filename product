from pyspark.sql.functions import countDistinct, sum, avg, expr, stddev, min, max, month, year

def advanced_financial_metrics(df):
    return df.groupBy("prd_name").agg(
        stddev("curr_bal_amt").alias("stddev_current_balance"),
        min("curr_bal_amt").alias("min_current_balance"),
        max("curr_bal_amt").alias("max_current_balance"),
        stddev("ledger_bal_amt").alias("stddev_ledger_balance"),
        min("ledger_bal_amt").alias("min_ledger_balance"),
        max("ledger_bal_amt").alias("max_ledger_balance")
    )

def temporal_features(df):
    # Extract month and year from 'open_date'
    df = df.withColumn("month", month("open_date")) \
           .withColumn("year", year("open_date"))
    
    # Group by 'prd_name', pivot on 'month', and calculate averages and counts
    return df.groupBy("prd_name").pivot("month").agg(
        avg("curr_bal_amt").alias("month_avg_monthly_balance"),
        countDistinct("ip_id").alias("month_count_users_opened")
    ).select(
        "prd_name",
        *[expr(f"`{c}` as `month_{c.split('_')[0]}_{c.split('_')[1]}`") for c in df.columns if c.startswith('avg') or c.startswith('count')]
    )

def interaction_intensity_features(df):
    return df.groupBy("prd_name").agg(
        expr("COUNT(ip_id) / COUNT(DISTINCT ip_id)").alias("avg_interactions_per_user")
    )

def aggregate_product_features(spark, df):
    df_basic = count_users_per_product(df)
    df_financial = financial_metrics_per_product(df)
    df_advanced_financial = advanced_financial_metrics(df)
    df_temporal = temporal_features(df)
    df_interaction = interaction_intensity_features(df)
    df_average_duration = average_open_duration_per_product(df)

    # Join all feature dataframes on 'prd_name'
    df_final = df_basic.join(df_financial, "prd_name", "outer") \
                       .join(df_advanced_financial, "prd_name", "outer") \
                       .join(df_temporal, "prd_name", "outer") \
                       .join(df_interaction, "prd_name", "outer") \
                       .join(df_average_duration, "prd_name", "outer")
    return df_final

# Example usage
spark = generate_spark_instance(total_memory=300, total_vcpu=150)
df = spark.sql("select * from dm_r3.pwm_mstr_dtl_daily")
product_features_df = aggregate_product_features(spark, df)
product_features_df.show()



